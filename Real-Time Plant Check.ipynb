{"cells":[{"cell_type":"markdown","metadata":{"id":"H7qp8oy4jaha"},"source":["# **Real-Time Plant Health Check!**"]},{"cell_type":"markdown","metadata":{"id":"NRHPAqlKjahe"},"source":["Having built our classification model by using MobileNetV2 as our base of CNN, we will not try to load and implement the same model to perform classification real-time using Webcamera and Android mobile camera"]},{"cell_type":"markdown","metadata":{"id":"EoeK9W2mjahf"},"source":["# Import all the modules"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sh7jvKd5jahg","outputId":"b6625205-c1e3-48e0-aac8-3d3e5e65c8c7"},"outputs":[{"data":{"text/plain":["'2.7.0'"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["import keras\n","keras.__version__"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tsW-afFnIjdT"},"outputs":[],"source":["import cv2\n","from PIL import Image\n","import os\n","import random\n","import numpy as np\n","from matplotlib import pyplot as plt\n","import urllib.request\n","\n","import tensorflow as tf\n","from keras.preprocessing import image\n","from keras import models\n","from keras import layers\n","from keras import optimizers\n","from tensorflow.keras.callbacks import EarlyStopping\n","from keras.preprocessing.image import ImageDataGenerator"]},{"cell_type":"markdown","metadata":{"id":"F_hvpxmed0tw"},"source":["***********************************************************************************************\n"]},{"cell_type":"markdown","metadata":{"id":"1XEVRSNdjahj"},"source":["# **Implement Model to perform real-time classification**\n","\n","We will impement model on webcam and android camera. Unfortunately I have not been able to access IOS cameras because of the security encryption and permissions. Furthermore, the interface still requires laptops to function and run prediction. Using Kivy to develop a UI with python to make an applcation could be one way to improve this and will be undertaken as a separate project.\n","\n","That said, lets with the integration!\n","\n","Lets first load our trained model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hLEoN6j2jahk"},"outputs":[],"source":["# Load the model that we just trained\n","model = keras.models.load_model('C:/Users/rishi/Documents/MSBAPM/SEM 4/Deep Learning/CNN Project/model_3.h5')"]},{"cell_type":"markdown","metadata":{"id":"TRZnsMRmjahl"},"source":["We will first need to create a directory to store the images captured by our cameras\n","\n","**NOTE**\n","\n","Please make the following folders before you run the following code\n","\n","* Main path \"Path\" and store the h5 file in it\n","* Folder \"Path/Data/\"\n","* Sub folder \"Path/Data/input_image_android/\" and \"Path/Data/input_image_web/\" for the screen grabs "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z5kLAW_9jahl"},"outputs":[],"source":["capture_data = 'C:/Users/rishi/Documents/MSBAPM/SEM 4/Deep Learning/CNN Project/Data/'"]},{"cell_type":"markdown","metadata":{"id":"CZ4JDLO5jahm"},"source":[">These images need to be processed to make them compatible to act as our input for prediction. I am creating a helper function that I will leverage while making predictions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rf6Bf0E2jahm"},"outputs":[],"source":["def preprocess(file_path):\n","    \n","    # Read in image from file path\n","    byte_img = tf.io.read_file(file_path)\n","    # Load in the image \n","    img = tf.io.decode_jpeg(byte_img)\n","    \n","    # Preprocessing steps - resizing the image to be 160x160x3\n","    img = tf.image.resize(img, (160,160))\n","    # Scale image to be between 0 and 1 \n","    img = img / 255.0\n","    \n","    \n","    # Return image\n","    return img"]},{"cell_type":"markdown","metadata":{"id":"tBSqmJpPjahn"},"source":["_________________________"]},{"cell_type":"markdown","metadata":{"id":"zjXnGPy-jahn"},"source":["# **Next the main crux of this project!! Integrating our laptop or mobile camera**\n","\n"," For the first part, I will integrate this with just the webcam as this solution will be usefull for those without an Android phone. For my use case this is not the best option but for other practical business problems such as Facial Verification and Face tagging, this would be all you need! \n"," \n"," But before we dive in, lets get familiar with openCV to understand what exactly is happening here."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IpZT659Bjahn"},"outputs":[],"source":["cap = cv2.VideoCapture(0)   # Start an Open CV instance that accesses your webcam. Change from 0 if there are multiple cameras\n","while cap.isOpened():  # While camera is open\n","    ret, frame = cap.read() # get the frame that is being captured\n","    frame = frame[120:120+250,200:200+250, :]  # Resize the frame to record only a section of the screen(here it is 250x250)\n","    \n","    cv2.imshow('Verification', frame)  # Show our frame on screen\n","    \n","    if cv2.waitKey(1) & 0xFF == ord('q'):  # wait for 1 millisecond after hitting 'q' to exit\n","        break\n","cap.release()   # release the webcam\n","cv2.destroyAllWindows()  # stop presenting the frame "]},{"cell_type":"markdown","metadata":{"id":"d_60C-c4jaho"},"source":["## **With a similar trigget key, we can do the following**\n","\n","1. Capture a single frame\n","1. Store it to our directory\n","1. Read it and process it in model compatible format\n","1. Evaluate and predict class\n","1. Print class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5e9hri-Hd2Zh","outputId":"5255c7fc-6b79-4e26-9497-da256e7cfa8f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Unhealthy\n","Unhealthy\n","Unhealthy\n","Unhealthy\n","Unhealthy\n"]}],"source":["cap = cv2.VideoCapture(0)\n","while cap.isOpened():\n","    ret, frame = cap.read()\n","    frame = frame[120:120+250,200:200+250, :]\n","    \n","    cv2.imshow('Verification', frame)\n","    \n","    # Verification trigger\n","    if cv2.waitKey(1) & 0xFF == ord('v'):\n","        # Save input image to capture_data/input_image folder \n","        cv2.imwrite(os.path.join(capture_data, 'input_image_web', 'input_image.jpg'), frame) # Write the screen grab to our directory\n","        \n","        # Run model\n","        grab = preprocess(os.path.join(capture_data, 'input_image_web', 'input_image.jpg')) # Make image compatible with model\n","        grab = tf.reshape(grab, (1,160,160,3))\n","        pred = np.round(model.predict(grab),0) # predict the probability and then predict class from probability\n","        if pred==0:\n","            print('Healthy')\n","        else:\n","            print('Unhealthy')\n","        \n","    \n","    if cv2.waitKey(1) & 0xFF == ord('q'):\n","        break\n","cap.release()\n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{"id":"GRlTujajjahp"},"source":["----------------------------------------------------------"]},{"cell_type":"markdown","source":["# **Accesing Mobile Camera to perform classification**"],"metadata":{"id":"zhLzdegGkiNM"}},{"cell_type":"markdown","metadata":{"id":"dwfo9tYnjahp"},"source":["This is great but not commpletely practical. No one is carrying around their laptops to identify. A work around to this is using an application that streams video capture screen of your mobile to a website and then get access to this stream to get a snapshot of our camera screen. \n","I will try to access our mobiles camera to perform our prediction. We will need to do the following as a part of our setup.\n","\n","**The following steps and chunk of code will require access to your IP address, Please use it at your own discretion**\n","\n","1. On your Android Device, download the IP Webcam application\n","> Link:- https://play.google.com/store/apps/details?id=com.pas.webcam&hl=en_US&gl=US\n","\n","1. Scroll down and start server. Copy the IP address found at the bottom of your screen\n","\n","1. Paste it in the folowing block of code\n","\n","\n","For experimentation, try the next code out and have fun!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KEAjWKP2jahq"},"outputs":[],"source":["URL = \"http://PASTE YOUR IP ADDRESS HERE/shot.jpg\"\n","\n","while True:\n","    img_arr = np.array(bytearray(urllib.request.urlopen(URL).read()),dtype=np.uint8)\n","    img = cv2.imdecode(img_arr,-1)\n","    img = img[340:340+400,760:760+400, :]\n","    img = np.rot90(img, k=3, axes=(0, 1))\n","    cv2.imshow('IPWebcam',img)\n","    \n","    q = cv2.waitKey(1)\n","    if q == ord(\"q\"):\n","        break;\n","\n","    \n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{"id":"raXmRmK5jahq"},"source":["## **Predict Cases**\n","Now lets get to business, the following code will use the same concept but this time we will introduce another key to grab a stil image from our stream. Not the fanciest ways to capture a picture but it suffices for the scope of this project. After this image is captured, we would like to read it and pass it through our model to give us a prediction. This is done as follows"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vqd3nEKljahq"},"outputs":[],"source":["URL = \"http://PASTE YOUR IP ADDRESS HERE/shot.jpg\"\n","\n","while True:\n","    img_arr = np.array(bytearray(urllib.request.urlopen(URL).read()),dtype=np.uint8)\n","    img = cv2.imdecode(img_arr,-1)\n","    img = img[340:340+400,760:760+400, :]\n","    img = np.rot90(img, k=3, axes=(0, 1))\n","    cv2.imshow('IPWebcam',img)\n","    \n","    v = cv2.waitKey(1)\n","    if v == ord(\"v\"):     # Use key \"v\" to capture an image\n","        cv2.imwrite(os.path.join(application_data, 'input_image_android', 'input_image.jpg'), img) # Write the screen grab to our directory\n","        \n","        # Run model\n","        grab_2 = preprocess(os.path.join(application_data, 'input_image_android', 'input_image.jpg')) # Make image compatible with model\n","        grab_2 = tf.reshape(grab_2, (1,160,160,3)) # reshape it as a single image input \n","        pred = np.round(model.predict(grab_2),0) # predict the probability and then predict class from probability\n","        if pred==0: # Print predicted class\n","            print('Healthy') \n","        else:\n","            print('Unhealthy')\n","            \n","    \n","    q = cv2.waitKey(1)\n","    if q == ord(\"q\"):\n","        break;\n","\n","    \n","cv2.destroyAllWindows()"]},{"cell_type":"markdown","metadata":{"id":"1AfsTpcGjahr"},"source":["# **Improvements**\n","\n","Model was trained initially using MobileNetV2 under the asssumption that we would need to deploy it on mobile processors. But with the current application, we can experiment with more involved pretrained models."]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Real-Time Plant Check.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}